# Word2VecNNModel
Vector Representation of Words based on Neural Network models.

## Synopsis

<!-- These Jupyter Notebooks are made as I am learning Neural Networks. The order in which the notebooks should be read is given below. Anyone is welcome to make suggestions and edits to notebooks. I believe that they can be helpful to anyone who is getting started in this field. -->

## General Format

<!-- Each notebook consists of implementation of some learning algorithm for neural networks or representation tools like matplotlib, performance surfaces, etc. -->

## Motivation

<!-- I'm putting these notebooks online so anyone can look them up as reference to Neural Network implementations. Again, just to mention, these cover material from very basic level to some advanced algorithms. (Please refer to the list given below.) -->

## Viewing Jupyter Notebooks in "nbviewer"

<!-- You can view these notebooks in nbviewer here: http://nbviewer.jupyter.org/github/KrnTneja/NeuralNetworkNotebooks/tree/master/ -->

## Imports

<!-- All the imports used in a notebook are given at the top of the notebook. Any errors coming up on your machine can generally be due missing libraries. Please look up at top of notebook in that case and install required libraries/modules. -->

## Tests

<!-- For all the functions and classes defined in notebook, they have a test example that I've kept as a standard example to test whenever I make a change to function or class. You can look them up as examples or tests for checking the working on you machine. -->

## Concepts

<!-- Most of the concepts have been explained in the notebook before they are implemented. Latex has been exploited for the best experience for the reader. -->

## Comments

<!-- Comments after 'def' line of a function explains what the function is made to do. It also explains the format of input function expects and the output one must expect from function. -->

## Licence

<!--Anyone can use these notebooks for educational purposes (but not for any commercial purposes or similar reasons whatsoever). -->

## Contents

<!-- <ul>
<li><a href="https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Transfer%20Functions.ipynb">Transfer Functions.ipynb</a></li>
<li><a href="https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Simple%20Layers%20of%20Neurons.ipynb">Simple Layers of Neurons.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Recurrent%20Networks.ipynb">Recurrent Networks.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Perceptron%20Learning%20Algorithm%20and%20Numpy%20Practice.ipynb">Perceptron Learning Algorithm and Numpy Practice.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/matplotlib%20Practice.ipynb">matplotlib Practice.ipyn</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/matplotlib%20Practice%203D.ipynb">matplotlib Practice 3D.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Supervised%20Hebbian%20Learning.ipynb">Supervised Hebbian Learning.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Widrow-Hoff%20Learning.ipynb">Widrow-Hoff Learning.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Performance%20Optimization.ipynb">Performance Optimization.ipynb</a></li>
<li><a href= "https://github.com/KrnTneja/NeuralNetworkNotebooks/blob/master/Backpropagation.ipynb">Backpropagation.ipynb</a></li>
<li>Fruit Classification using Backpropagation.ipynb</li> 
</ul> -->

## Articles, Papers and Books
<ul>
<li>[Article] <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">The amazing power of word vectors</a></li>
<li>[Paper] <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a></li>
<li>[Paper] <a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
<li>[Paper] <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li>[Paper] <a href="http://msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf">Linguistic Regularities in Continuous Space Word Representations</a></li>
<li>[Paper] <a href="https://arxiv.org/pdf/1411.2738v3.pdf">word2vec Parameter Learning Explained</a></li>
<li>[Paper] <a href="https://arxiv.org/pdf/1402.3722v1.pdf">word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method</a></li>
<li>[StackOverflow] <a href="http://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function"></a></li>
<!--<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>
<li>[Paper] <a href=""></a></li>-->
</ul>

## Random Ideas
<ul>
<li>Definitions and dictionary elements can be a good source of understanding near-perfect semantic and syntactic relationships.</li>
</ul>




